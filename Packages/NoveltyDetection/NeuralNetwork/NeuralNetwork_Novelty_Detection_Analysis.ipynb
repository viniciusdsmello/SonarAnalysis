{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Time to read data file: 1.0206046104431152 seconds\n",
      "Qtd event of A is 12939\n",
      "Qtd event of B is 29352\n",
      "Qtd event of C is 11510\n",
      "Qtd event of D is 23760\n",
      "\n",
      "Biggest class is B with 29352 events\n",
      "Total of events in the dataset is 77561\n",
      "Balacing data...\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (12939, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (29352, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (11510, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (23760, 400)\n",
      "Reading from /home/vinicius.mello/Workspace/LPS/SonarAnalysis/Results/NoveltyDetection/4_folds_cross_validation_balanced_data.jbl\n",
      "Reading from /home/vinicius.mello/Workspace/LPS/SonarAnalysis/Results/NoveltyDetection/4_folds_cross_validation_balanced_data.jbl\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "import pprint \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from Packages.NoveltyDetection.setup.noveltyDetectionConfig import CONFIG\n",
    "from NNNoveltyDetectionAnalysis import NNNoveltyDetectionAnalysis\n",
    "from Functions.telegrambot import Bot\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "my_bot = Bot(\"lisa_thebot\")\n",
    "\n",
    "# Enviroment variables\n",
    "data_path = CONFIG['OUTPUTDATAPATH']\n",
    "results_path = CONFIG['PACKAGE_NAME']\n",
    "\n",
    "training_params = {\n",
    "    \"Technique\": \"NeuralNetwork\",\n",
    "    \"DevelopmentMode\": False,\n",
    "    \"DevelopmentEvents\": 1600,\n",
    "    \"NoveltyDetection\": True,\n",
    "    \"InputDataConfig\": {\n",
    "        \"database\": \"4classes\",\n",
    "        \"n_pts_fft\": 1024,\n",
    "        \"decimation_rate\": 3,\n",
    "        \"spectrum_bins_left\": 400,\n",
    "        \"n_windows\": 1,\n",
    "        \"balance_data\": True\n",
    "    },\n",
    "    \"OptmizerAlgorithm\": {\n",
    "        \"name\": \"Adam\",\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"beta_1\": 0.90,\n",
    "            \"beta_2\": 0.999,\n",
    "            \"epsilon\": 1e-08,\n",
    "            \"learning_decay\": 1e-6,\n",
    "            \"momentum\": 0.3,\n",
    "            \"nesterov\": True\n",
    "        }\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"n_folds\": 4,\n",
    "        \"n_epochs\": 300,\n",
    "        \"n_inits\": 2,\n",
    "        \"batch_size\": 128,\n",
    "        \"kernel_initializer\": \"uniform\",\n",
    "        \"hidden_activation_function\": \"tanh\", #\"relu\",\n",
    "        \"classifier_output_activation_function\": \"softmax\",\n",
    "        \"norm\": \"mapstd\",\n",
    "        \"metrics\": [\"accuracy\"],\n",
    "        \"loss\": \"mean_squared_error\",\n",
    "        \"dropout\": False,\n",
    "        \"dropout_parameter\": 0.0,\n",
    "        \"regularization\": None,\n",
    "        \"regularization_parameter\": 0.0\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"EarlyStopping\": {\n",
    "            \"patience\": 50,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "analysis = NNNoveltyDetectionAnalysis(parameters=training_params, model_hash=\"\", load_hash=False, load_data=True, verbose=True)\n",
    "all_data, all_trgt, all_trgt_sparse = analysis.getData()\n",
    "\n",
    "trn_data = analysis.trn_data\n",
    "trn_trgt = analysis.trn_trgt\n",
    "trn_trgt_sparse = analysis.trn_trgt_sparse\n",
    "\n",
    "models = analysis.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d7175ec280d30121856e19c075f00e5c48b862c9397484062d6c2612256f8198\n",
      "/home/vinicius.mello/Workspace/LPS/SonarAnalysis/Results/NoveltyDetection/NeuralNetwork/outputs/d7175ec280d30121856e19c075f00e5c48b862c9397484062d6c2612256f8198\n",
      "{'DevelopmentEvents': 1600,\n",
      " 'DevelopmentMode': False,\n",
      " 'HyperParameters': {'batch_size': 128,\n",
      "                     'classifier_output_activation_function': 'softmax',\n",
      "                     'dropout': False,\n",
      "                     'dropout_parameter': 0.0,\n",
      "                     'hidden_activation_function': 'tanh',\n",
      "                     'kernel_initializer': 'uniform',\n",
      "                     'loss': 'mean_squared_error',\n",
      "                     'metrics': ['accuracy'],\n",
      "                     'n_epochs': 300,\n",
      "                     'n_folds': 4,\n",
      "                     'n_inits': 2,\n",
      "                     'norm': 'mapstd',\n",
      "                     'regularization': None,\n",
      "                     'regularization_parameter': 0.0},\n",
      " 'InputDataConfig': {'balance_data': True,\n",
      "                     'database': '4classes',\n",
      "                     'decimation_rate': 3,\n",
      "                     'n_pts_fft': 1024,\n",
      "                     'n_windows': 1,\n",
      "                     'spectrum_bins_left': 400},\n",
      " 'NoveltyDetection': True,\n",
      " 'OptmizerAlgorithm': {'name': 'Adam',\n",
      "                       'parameters': {'beta_1': 0.9,\n",
      "                                      'beta_2': 0.999,\n",
      "                                      'epsilon': 1e-08,\n",
      "                                      'learning_decay': 1e-06,\n",
      "                                      'learning_rate': 0.001,\n",
      "                                      'momentum': 0.3,\n",
      "                                      'nesterov': True}},\n",
      " 'Technique': 'NeuralNetwork',\n",
      " 'callbacks': {'EarlyStopping': {'monitor': 'val_loss', 'patience': 50}}}\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "print(analysis.model_hash)\n",
    "print(analysis.getBaseResultsPath())\n",
    "pp.pprint(analysis.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python neuralnetwork_train.py --layer 1 --novelty 0 --threads 4 --type neuronSweep --hiddenNeurons 100 --neuronsVariationStep 10 --modelhash d7175ec280d30121856e19c075f00e5c48b862c9397484062d6c2612256f8198\n",
      "The training of the model for novelty class A took 0:00:19.331133 to be performed\n",
      "\n",
      "python neuralnetwork_train.py --layer 1 --novelty 1 --threads 4 --type neuronSweep --hiddenNeurons 100 --neuronsVariationStep 10 --modelhash d7175ec280d30121856e19c075f00e5c48b862c9397484062d6c2612256f8198\n",
      "The training of the model for novelty class B took 0:00:19.625044 to be performed\n",
      "\n",
      "python neuralnetwork_train.py --layer 1 --novelty 2 --threads 4 --type neuronSweep --hiddenNeurons 100 --neuronsVariationStep 10 --modelhash d7175ec280d30121856e19c075f00e5c48b862c9397484062d6c2612256f8198\n",
      "The training of the model for novelty class C took 0:00:16.716187 to be performed\n",
      "\n",
      "python neuralnetwork_train.py --layer 1 --novelty 3 --threads 4 --type neuronSweep --hiddenNeurons 100 --neuronsVariationStep 10 --modelhash d7175ec280d30121856e19c075f00e5c48b862c9397484062d6c2612256f8198\n",
      "The training of the model for novelty class D took 0:00:17.144136 to be performed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inovelty in range(len(analysis.class_labels)):\n",
    "    startTime = time.time()\n",
    "    \n",
    "    analysis.train(layer=1,\n",
    "                   inovelty=inovelty,\n",
    "                   trainingType=\"neuronSweep\", #foldSweep, neuronSweep, normal\n",
    "                   hidden_neurons=[100],\n",
    "                   neurons_variation_step=10,\n",
    "                   numThreads=4,\n",
    "                   model_hash=analysis.model_hash)\n",
    "    \n",
    "    duration = str(timedelta(seconds=float(time.time() - startTime)))\n",
    "    print(\"The training of the model for novelty class {0} took {1} to be performed\\n\".format(analysis.class_labels[inovelty], duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures-of-Merit Analysis for a threshold variation at output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty class: 0 - Topology: 400x400x300x200x3 - fold 0\n",
      "Neural Network - Layer: 3 - Topology: 400x400x300x200 - Fold 1 of 4 Folds -  Init 1 of 2 Inits\n",
      "Train on 66042 samples, validate on 22014 samples\n",
      "Epoch 1/300\n",
      "66042/66042 [==============================] - 3s 50us/step - loss: 0.0990 - acc: 0.7938 - val_loss: 0.0938 - val_acc: 0.8062\n",
      "Epoch 2/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0845 - acc: 0.8281 - val_loss: 0.0947 - val_acc: 0.8055\n",
      "Epoch 3/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0703 - acc: 0.8598 - val_loss: 0.0876 - val_acc: 0.8238\n",
      "Epoch 4/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0557 - acc: 0.8914 - val_loss: 0.0913 - val_acc: 0.8167\n",
      "Epoch 5/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0425 - acc: 0.9198 - val_loss: 0.0817 - val_acc: 0.8422\n",
      "Epoch 6/300\n",
      "66042/66042 [==============================] - 3s 44us/step - loss: 0.0320 - acc: 0.9419 - val_loss: 0.0753 - val_acc: 0.8572\n",
      "Epoch 7/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0239 - acc: 0.9578 - val_loss: 0.0705 - val_acc: 0.8709\n",
      "Epoch 8/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0193 - acc: 0.9664 - val_loss: 0.0730 - val_acc: 0.8676\n",
      "Epoch 9/300\n",
      "66042/66042 [==============================] - 3s 43us/step - loss: 0.0157 - acc: 0.9731 - val_loss: 0.0751 - val_acc: 0.8654\n",
      "Epoch 10/300\n",
      "66042/66042 [==============================] - 3s 42us/step - loss: 0.0139 - acc: 0.9768 - val_loss: 0.0772 - val_acc: 0.8643\n",
      "Epoch 11/300\n",
      "66042/66042 [==============================] - 3s 42us/step - loss: 0.0131 - acc: 0.9776 - val_loss: 0.0772 - val_acc: 0.8635\n",
      "Epoch 12/300\n",
      "10368/66042 [===>..........................] - ETA: 2s - loss: 0.0110 - acc: 0.9805"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2986c7e2d619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                                                         ifold))\n\u001b[1;32m     55\u001b[0m                 classifier = models[inovelty].get_model(data=analysis.trn_data[inovelty], trgt=analysis.trn_trgt[inovelty],\n\u001b[0;32m---> 56\u001b[0;31m                                                         hidden_neurons=hidden_neurons, layer=layer, ifold=ifold)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# normalize known classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/LPS/SonarAnalysis/Functions/NeuralNetworks.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, data, trgt, hidden_neurons, layer, ifold)\u001b[0m\n\u001b[1;32m    116\u001b[0m                        \u001b[0mifold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mifold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                        \u001b[0mhidden_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_neurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                        \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                        )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/LPS/SonarAnalysis/Functions/NeuralNetworks.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, trgt, ifold, hidden_neurons, layer)\u001b[0m\n\u001b[1;32m    235\u001b[0m                                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrgt_sparse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                                       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                                       \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                                       )\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_trn_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sonarenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/sonarenv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sonarenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sonarenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sonarenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Thresolds variation x Figures of Merit\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from Functions.StatisticalAnalysis import KLDiv, EstPDF\n",
    "from Functions import FunctionsDataVisualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose layer \n",
    "layer = 3\n",
    "\n",
    "# Choose neurons topology\n",
    "neurons_mat = [1] + list(range(10,110,10))\n",
    "neurons_mat = [1]\n",
    "\n",
    "for ineuron in neurons_mat:\n",
    "    hidden_neurons = [400,300,200]\n",
    "    neurons_str = models[0].get_neurons_str(trn_data[0],hidden_neurons=hidden_neurons)\n",
    "    analysis_name = 'figures_of_merit_{}_layer_{}_neurons'.format(layer,neurons_str)\n",
    "    analysis_file = os.path.join(analysis.getBaseResultsPath(), \"AnalysisFiles\", analysis_name + \".jbl\")    \n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    # Plot parameters\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 14\n",
    "    plt.rcParams['ytick.labelsize'] = 14\n",
    "    plt.rcParams['legend.numpoints'] = 1\n",
    "    plt.rcParams['legend.handlelength'] = 3\n",
    "    plt.rcParams['legend.borderpad'] = 0.3\n",
    "    plt.rcParams['legend.fontsize'] = 14\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "    figsize = (20,15)\n",
    "\n",
    "\n",
    "    if not os.path.exists(analysis_file):\n",
    "        thr_mat = np.round(np.arange(0.0,1.05,0.05),3)\n",
    "        thr_mat[thr_mat>-0.1] = abs(thr_mat[thr_mat>-0.1])\n",
    "        n_folds = analysis.parameters[\"HyperParameters\"][\"n_folds\"]\n",
    "        class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(np.unique(all_trgt)),len(thr_mat)])\n",
    "        novelty_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "        known_acc_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "        known_sp_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "        known_trig_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "\n",
    "        for inovelty, novelty_class in enumerate(np.unique(analysis.all_trgt)):\n",
    "            for ifold in range(len(analysis.CVO[inovelty])):\n",
    "                train_id, test_id = analysis.CVO[inovelty][ifold]\n",
    "\n",
    "                print('Novelty class: %01.0f - Topology: %s - fold %i'%(novelty_class,\n",
    "                                                                        models[inovelty].get_neurons_str(data=trn_data[inovelty], hidden_neurons=hidden_neurons)+'x'+str(trn_trgt_sparse[inovelty].shape[1]),\n",
    "                                                                        ifold))\n",
    "                classifier = models[inovelty].get_model(data=analysis.trn_data[inovelty], trgt=analysis.trn_trgt[inovelty],\n",
    "                                                        hidden_neurons=hidden_neurons, layer=layer, ifold=ifold)\n",
    "\n",
    "                # normalize known classes\n",
    "                if analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd\":\n",
    "                    scaler = preprocessing.StandardScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapstd_rob\":\n",
    "                    scaler = preprocessing.RobustScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "                elif analysis.parameters[\"HyperParameters\"][\"norm\"] == \"mapminmax\":\n",
    "                    scaler = preprocessing.MinMaxScaler().fit(all_data[all_trgt!=inovelty][train_id,:])\n",
    "\n",
    "                known_data = scaler.transform(analysis.trn_data[inovelty][test_id,:])\n",
    "                known_target = analysis.trn_trgt[inovelty][test_id]\n",
    "\n",
    "                novelty_data = scaler.transform(all_data[all_trgt==inovelty])\n",
    "\n",
    "                known_output = classifier.predict(known_data)\n",
    "                novelty_output = classifier.predict(novelty_data)\n",
    "\n",
    "                for ithr,thr_value in enumerate(thr_mat): \n",
    "                    buff = np.zeros([len(np.unique(analysis.all_trgt))-1])\n",
    "                    for iclass, class_id in enumerate(np.unique(analysis.all_trgt)):\n",
    "                        if iclass == inovelty:\n",
    "                            continue\n",
    "                        output_of_class_events = known_output[known_target==iclass-(iclass>inovelty),:]\n",
    "                        correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                        output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                        class_eff_mat[ifold, inovelty, iclass, ithr] = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                        buff[iclass-(iclass>inovelty)] = class_eff_mat[ifold, inovelty, iclass, ithr]\n",
    "\n",
    "                    novelty_eff_mat[ifold, inovelty, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                    known_acc_mat[ifold, inovelty, ithr] = np.mean(buff,axis=0)\n",
    "                    known_sp_mat[ifold, inovelty, ithr]= (np.sqrt(np.mean(buff,axis=0)*np.power(np.prod(buff),1./float(len(buff)))))\n",
    "                    known_trig_mat[ifold, inovelty, ithr]=float(sum(np.max(known_output,axis=1)>thr_value))/float(len(known_output))\n",
    "\n",
    "    #             class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat = analysis.get_figures_of_merit(known_output=output,\n",
    "    #                                                                                                                         known_target=known_trgt, \n",
    "    #                                                                                                                         novelty_output=novelty_output,\n",
    "    #                                                                                                                         thr_mat=thr_mat, \n",
    "    #                                                                                                                         inovelty=inovelty,\n",
    "    #                                                                                                                         ifold=ifold)\n",
    "        joblib.dump([class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat, thr_mat],\n",
    "                    analysis_file,compress=9)\n",
    "    else:\n",
    "        print('file exists')\n",
    "        [class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat, thr_mat] = joblib.load(analysis_file) \n",
    "\n",
    "    # plot analysis\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline  \n",
    "\n",
    "    fig = plt.subplots(figsize=figsize)\n",
    "\n",
    "    for inovelty, novelty_class in enumerate(np.unique(all_trgt)):\n",
    "        ax = plt.subplot(2,2,inovelty+1)\n",
    "        for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "            if novelty_class == m_class:\n",
    "                #a = 0\n",
    "                ax.errorbar(thr_mat,np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0),\n",
    "                            np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0),fmt='o-',\n",
    "                            color='k',alpha=0.7,linewidth=3,\n",
    "                            label='Det. Novidade')\n",
    "                ax.errorbar(thr_mat,np.mean(known_acc_mat[:,int(novelty_class),:],axis=0),\n",
    "                            np.std(known_acc_mat[:,int(novelty_class),:],axis=0),fmt='o--',\n",
    "                            color='m',alpha=0.7,linewidth=3,\n",
    "                            label='Acurácia')\n",
    "                ax.errorbar(thr_mat,np.mean(known_sp_mat[:,int(novelty_class),:],axis=0),\n",
    "                            np.std(known_sp_mat[:,int(novelty_class),:],axis=0),fmt='o:',\n",
    "                            color='c',alpha=0.7,linewidth=3,\n",
    "                            label='Índice SP')\n",
    "                ax.errorbar(thr_mat,np.mean(known_trig_mat[:,int(novelty_class),:],axis=0),\n",
    "                            np.std(known_trig_mat[:,int(novelty_class),:],axis=0),fmt='o-.',\n",
    "                            color='k',alpha=0.7,linewidth=3,\n",
    "                            label='Trigger')\n",
    "            else:\n",
    "                ax.errorbar(thr_mat,np.mean(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),\n",
    "                            np.std(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),fmt='o-',\n",
    "                            color=m_colors[int(m_class)],alpha=0.7,linewidth=3,\n",
    "                           label='Eficiência Classe {}'.format(analysis.getClassLabels()[iclass]))\n",
    "        ax.set_xticks(thr_mat)\n",
    "        ax.set_xticklabels(thr_mat,rotation=45, fontsize=18)\n",
    "        ax.set_title('MLP - Topologia {} - Classe {} como Novidade'.format(neurons_str, analysis.getClassLabels()[inovelty]),fontsize=18,weight='bold')\n",
    "        ax.set_xlim([np.min(thr_mat), np.max(thr_mat)])\n",
    "\n",
    "        ax.set_ylim([0.0, 1.3])\n",
    "        y_ticks = np.arange(0.0,1.3,0.1)\n",
    "        ax.set_yticks(y_ticks)\n",
    "        y_tick_labels = 100*y_ticks[y_ticks<=1.0]\n",
    "        y_tick_labels = y_tick_labels.astype(int)\n",
    "        ax.set_yticklabels(y_tick_labels,fontsize=18)\n",
    "\n",
    "        ax.grid()\n",
    "\n",
    "        if inovelty > 1:\n",
    "            ax.set_xlabel('Limiar',fontsize=18,weight='bold')\n",
    "        if inovelty == 0 or inovelty == 2:\n",
    "            ax.set_ylabel('Figuras de Mérito (%)',fontsize=18,weight='bold')\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # sort both labels and handles by labels\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "        ax.legend(handles, labels, ncol=3, loc='upper center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        rect = [0.1, 0.2, 0.3, 0.4]\n",
    "        ax1 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "        a = thr_mat>=0.8\n",
    "        b = thr_mat<=1.0\n",
    "        selected_thr = a & b\n",
    "        \n",
    "        ax1.errorbar(thr_mat[selected_thr],np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0)[selected_thr],\n",
    "                    np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0)[selected_thr],fmt='o-',\n",
    "                    color='k',alpha=0.7,linewidth=3,\n",
    "                    label='Det. Novidade')\n",
    "        ax1.errorbar(thr_mat[selected_thr],np.mean(known_sp_mat[:,int(novelty_class),:],axis=0)[selected_thr],\n",
    "                    np.std(known_sp_mat[:,int(novelty_class),:],axis=0)[selected_thr],fmt='o:',\n",
    "                    color='c',alpha=0.7,linewidth=3,\n",
    "                    label='Índice SP')\n",
    "        ax1.errorbar(thr_mat[selected_thr],np.mean(known_trig_mat[:,int(novelty_class),:],axis=0)[selected_thr],\n",
    "                    np.std(known_trig_mat[:,int(novelty_class),:],axis=0)[selected_thr],fmt='o-.',\n",
    "                    color='k',alpha=0.7,linewidth=3,\n",
    "                    label='Trigger')\n",
    "        \n",
    "        ax1.set_xticks(thr_mat[selected_thr])\n",
    "        ax1.set_xticklabels(thr_mat[selected_thr],rotation=45, fontsize=22)\n",
    "\n",
    "        ax1.set_ylim([0.0, 0.9])\n",
    "        y_ticks = np.arange(0.0,1.1,0.1)\n",
    "        y_ticks = np.round(y_ticks,2)\n",
    "        ax1.set_yticks(y_ticks)\n",
    "        ax1.set_yticklabels(100.*ax1.get_yticks(), fontsize=22)\n",
    "\n",
    "        ax1.grid()\n",
    "\n",
    "     #Save the figure\n",
    "    file_name = os.path.join(analysis.pictures_output_folder, analysis_name+\"_{}_novelty_{}_neurons.png\".format(inovelty,neurons_str))\n",
    "    plt.savefig(file_name, format=\"png\")\n",
    "    try: \n",
    "        my_bot.sendMessage(imgPath=file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error when sending the image to the bot. Error: {}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# analysis example - novelty detection for neural network\n",
    "# thr. sweep\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import FunctionsDataVisualization\n",
    "\n",
    "ineuron = 20\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'figures_of_merit'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_%i_neurons_novelty_detection_thr_sweep.jbl'%(results_path,analysis_str,\n",
    "                                                                             analysis_name, \n",
    "                                                                             trn_params.get_params_str(),\n",
    "                                                                             ineuron)\n",
    "\n",
    "# if os.path.exists(analysis_file_name):\n",
    "#         os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "    \n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NNNoveltyDetectionTrnParams(n_epochs=30,n_inits=1)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NNNoveltyDetectionTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "\n",
    "    thr_mat = np.round(np.arange(-1.0,1.1,0.1),3)\n",
    "    thr_mat[thr_mat>-0.1] = abs(thr_mat[thr_mat>-0.1])\n",
    "\n",
    "    class_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    novelty_eff_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    known_acc_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    known_sp_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    known_trig_mat = np.zeros([n_folds,len(np.unique(all_trgt)),len(thr_mat)])\n",
    "    \n",
    "    for inovelty, novelty_class in enumerate(np.unique(trgt)):\n",
    "        trn_data = all_data[all_trgt!=novelty_class]\n",
    "        trn_trgt = all_trgt[all_trgt!=novelty_class]\n",
    "        # trgt max = 2\n",
    "        trn_trgt[trn_trgt>novelty_class] = trn_trgt[trn_trgt>novelty_class]-1\n",
    "        \n",
    "        for ifold in range(len(CVO[inovelty])):\n",
    "            print 'Novelty class: %01.0f - neuron: %i - fold %i'%(novelty_class, ineuron, ifold)\n",
    "            [classifier,trn_desc] = TrainFunctions.NNNoveltyTrainFunction(data=trn_data,\n",
    "                                                                          trgt=trn_trgt, \n",
    "                                                                          inovelty=inovelty, \n",
    "                                                                          ifold=ifold, \n",
    "                                                                          n_folds=len(CVO[inovelty]), \n",
    "                                                                          n_neurons=ineuron, \n",
    "                                                                          trn_params=trn_params, \n",
    "                                                                          save_path=results_path, \n",
    "                                                                          dev=development_flag)\n",
    "            \n",
    "            output = classifier.predict(trn_data)\n",
    "            novelty_output = classifier.predict(all_data[all_trgt==novelty_class])\n",
    "            for ithr,thr_value in enumerate(thr_mat): \n",
    "                buff = np.zeros([len(np.unique(trgt))-1])\n",
    "                for iclass, class_id in enumerate(np.unique(trgt)):\n",
    "                    if iclass == inovelty:\n",
    "                        continue\n",
    "                    output_of_class_events = output[trn_trgt==iclass-(iclass>inovelty),:]\n",
    "                    correct_class_output = np.argmax(output_of_class_events,axis=1)==iclass-(iclass>inovelty)\n",
    "                    output_above_thr = output_of_class_events[correct_class_output,iclass-(iclass>inovelty)]>thr_value\n",
    "                    class_eff_mat[ifold, inovelty, iclass, ithr] = float(sum(output_above_thr))/float(len(output_of_class_events))\n",
    "                    buff[iclass-(iclass>inovelty)] = class_eff_mat[ifold, inovelty, iclass, ithr]\n",
    "                novelty_eff_mat[ifold, inovelty, ithr] = float(sum(1-(novelty_output>thr_value).any(axis=1)))/float(len(novelty_output))\n",
    "                known_acc_mat[ifold, inovelty, ithr] = np.mean(buff,axis=0)\n",
    "                known_sp_mat[ifold, inovelty, ithr]= (np.sqrt(np.mean(buff,axis=0)\n",
    "                                                              *np.power(np.prod(buff),1./float(len(buff)))))\n",
    "                known_trig_mat[ifold, inovelty, ithr]=float(sum(np.max(output,axis=1)>thr_value))/float(len(output))\n",
    "    joblib.dump([class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat, thr_mat],\n",
    "                analysis_file_name,compress=9)\n",
    "else:\n",
    "    print 'file exists'\n",
    "    [class_eff_mat, novelty_eff_mat, known_acc_mat, known_sp_mat, known_trig_mat, thr_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.subplots(figsize=(20,15))\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "inovelty = 3\n",
    "\n",
    "ax = plt.subplot(2,2,inovelty+1)\n",
    "for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "    novelty_class = inovelty\n",
    "    if novelty_class == m_class:\n",
    "        #a = 0\n",
    "        ax.errorbar(thr_mat,np.mean(novelty_eff_mat[:,int(novelty_class),:],axis=0),\n",
    "                    np.std(novelty_eff_mat[:,int(novelty_class),:],axis=0),fmt='o-',\n",
    "                    color='k',alpha=0.7,linewidth=2.5,\n",
    "                    label='Novel Det.')\n",
    "        ax.errorbar(thr_mat,np.mean(known_acc_mat[:,int(novelty_class),:],axis=0),\n",
    "                    np.std(known_acc_mat[:,int(novelty_class),:],axis=0),fmt='o--',\n",
    "                    color='k',alpha=0.7,linewidth=2.5,\n",
    "                    label='Known Acc.')\n",
    "        ax.errorbar(thr_mat,np.mean(known_sp_mat[:,int(novelty_class),:],axis=0),\n",
    "                    np.std(known_sp_mat[:,int(novelty_class),:],axis=0),fmt='o:',\n",
    "                    color='k',alpha=0.7,linewidth=2.5,\n",
    "                    label='Known SP')\n",
    "    else:\n",
    "        ax.errorbar(thr_mat,np.mean(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),\n",
    "                    np.std(class_eff_mat[:,int(novelty_class),int(m_class),:],axis=0),fmt='o-',\n",
    "                    color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\n",
    "                   label='C%i Eff.'%(int(iclass)))\n",
    "ax.set_xticks(thr_mat)\n",
    "ax.set_xticklabels(thr_mat,rotation=45, fontsize=18)\n",
    "ax.set_title('Class %i as the novelty class'%inovelty,fontsize=18,weight='bold')\n",
    "ax.set_xlim([0, np.max(thr_mat)])\n",
    "\n",
    "ax.set_ylim([0.0, 1.3])\n",
    "y_ticks = np.arange(0.0,1.1,0.1)\n",
    "y_ticks = np.round(y_ticks,2)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(100*y_ticks[y_ticks<=1.0],fontsize=18)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "ax.set_xlabel('Threshold',fontsize=18,weight='bold')\n",
    "ax.set_ylabel('Figures-of-Merit (%)',fontsize=18,weight='bold')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# sort both labels and handles by labels\n",
    "labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "ax.legend(handles, labels, ncol=3, loc='upper center')\n",
    "rect = [0.1, 0.12, 0.5, 0.4]\n",
    "ax1 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "a = thr_mat>=0.7 \n",
    "b = thr_mat<=1\n",
    "selected_thr = a & b\n",
    "for iclass, m_class in enumerate(np.unique(all_trgt)):\n",
    "    novelty_class = inovelty\n",
    "    if novelty_class == m_class:\n",
    "        #a = 0\n",
    "        ax1.errorbar(thr_mat[selected_thr],np.mean(novelty_eff_mat[:,int(novelty_class),selected_thr],axis=0),\n",
    "                    np.std(novelty_eff_mat[:,int(novelty_class),selected_thr],axis=0),fmt='o-',\n",
    "                    color='k',alpha=0.7,linewidth=2.5,\n",
    "                    label='Novel Det.')\n",
    "    else:\n",
    "        ax1.errorbar(thr_mat[selected_thr],np.mean(class_eff_mat[:,int(novelty_class),int(m_class),selected_thr],axis=0),\n",
    "                    np.std(class_eff_mat[:,int(novelty_class),int(m_class),selected_thr],axis=0),fmt='o-',\n",
    "                    color=m_colors[int(m_class)],alpha=0.7,linewidth=2.5,\n",
    "                   label='C%i Eff.'%(int(m_class)+1))\n",
    "ax1.set_xticks(thr_mat[selected_thr])\n",
    "ax1.set_xticklabels(thr_mat[selected_thr],rotation=45, fontsize=12)\n",
    "\n",
    "ax1.set_ylim([0.0, 0.9])\n",
    "y_ticks = np.arange(0.0,1.1,0.1)\n",
    "y_ticks = np.round(y_ticks,2)\n",
    "ax1.set_yticks(y_ticks)\n",
    "ax1.set_yticklabels(100.*ax1.get_yticks(), fontsize=12)\n",
    "\n",
    "ax1.grid()\n",
    "#Save the figure\n",
    "file_name = pict_results_path+'/'+current_analysis+'_%i_novelty_%s_neurons_'%(inovelty,ineuron)+trn_params.get_params_str()+'.pdf'\n",
    "plt.savefig(file_name)\n",
    "\n",
    "\n",
    "print trn_params.get_params_str()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
